{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IL181.007 - Deep Learning - Assignment 1\n",
    "### _Or Segal_\n",
    "\n",
    "In this assignment, we will implement a simple neural network to predict the survival of passengers on board the Titanic using only Numpy (other libraries will be used when we'll get to the pre-processing, but that does not count..!). First, we will show the implementation of the forward calculation, gradient calculation, and the updating of the weights. We will then pre-process the titanic data from Kaggle, and use the network we have built to predict survival.\n",
    "\n",
    "Several tutorials were used as resources, all of their links can be found in the References."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "\n",
    "We will first implement the two activation functions that we will be using, sigmoid and ReLU, as well as their derivatives for the backward propagation. We will follow the notation from Goodfellow, Bengio, and Courville (2016).\n",
    "\n",
    "$$\\begin{align}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-x}} \\quad &\\Rightarrow \\quad \\frac{d}{dz} \\sigma(z) = \\sigma(z)(1 - \\sigma(z)) \\\\\n",
    "g(z) = max(0, z) \\quad &\\Rightarrow \\quad \\frac{d}{dz} g(z) = \\begin{cases}\n",
    "1, \\quad \\textrm{if } x>0 \\\\\n",
    "0, \\quad \\textrm{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def sigmoid_deriv(dA, z):\n",
    "    sig_z = sigmoid(z)\n",
    "    return dA * sig_z * (1 - sig_z)\n",
    "\n",
    "def relu_deriv(dA, z):\n",
    "    dz = np.array(dA, copy=True)\n",
    "    dz[z <= 0] = 0\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the cost function, using the cross entropy loss:\n",
    "$$J(\\Theta) = -\\mathbb{E}_{\\mathbf{x}, \\mathbf{y} \\sim \\hat{p}_{data}}\\log p_{model}(y | x)$$\n",
    "or in its binary form:\n",
    "$$J(\\Theta) = - \\frac{1}{m} \\sum^m_{i=0} \\Big( y^{(i)} \\log (\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\Big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(Y_hat, Y):\n",
    "    cost = -1/Y_hat.shape[1] * (np.dot(Y, np.log(Y_hat).T) + \n",
    "                                np.dot((1 - Y), np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the neural network as a class as it will be convenient for information storage accross functions and an easier operation using the methods. Each layer is added with the `add_layer` method, which stores its details in an architecture dictionary and initiates its weights randomly.\n",
    "\n",
    "The `forward_calculation` method follows the architecture dictionary and uses the activation functions that we have implemented (they are also copies as methods in the class). The vector of inputs $\\mathbf{x}$, is used to compute an affine transformation $\\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}$, which is then passed to the activation function.\n",
    "\n",
    "The `back_propagation` method follows the reversed order of the architecture and calculates the gradients of the loss with respect to the output of the layer, weights, and biases, and propagates to the next lower-level layer:\n",
    "$$\\begin{align}\n",
    "\\nabla_{\\mathbf{Z}^{[l]}} J &= \\nabla_{\\mathbf{A}^{[l]}} J \\cdot g'(\\mathbf{Z}^{[l]}) \\\\\n",
    "\\nabla_{\\mathbf{b}^{[l]}} J &= \\frac{1}{m} \\sum^m_{i=1} \\nabla_{\\mathbf{Z}^{[l](i)}} J \\\\\n",
    "\\nabla_{\\mathbf{W}^{[l]}} J &= \\frac{1}{m} \\nabla_{\\mathbf{Z}^{[l]}} J \\cdot \\mathbf{A}^{[l - 1]T}\\\\\n",
    "\\nabla_{\\mathbf{A}^{[l - 1]}} J &= \\mathbf{A}^{[l]T} \\cdot \\nabla_{\\mathbf{Z}^{[l]}} J\n",
    "\\end{align}$$\n",
    "\n",
    "Finally, the `train` method performs the entire calculation - forward calculation, back propagation, and updating of the parameters with a defined learning rate - and iterates over these stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.architecture = [] # Layer structure \n",
    "        self.paramValues = {} # Ongoing parameter values (W, b)\n",
    "        \n",
    "        self.randomSeed = 23\n",
    "    \n",
    "    def add_layer(self, inputSize, outputSize, activation):\n",
    "        np.random.seed(self.randomSeed)\n",
    "        \n",
    "        # Make sure that we support the activation function (relu or sigmoid for now)\n",
    "        if activation not in [\"relu\", \"sigmoid\"]:\n",
    "            raise Exception(\"The activation function is not supported\")\n",
    "        \n",
    "        # Store the details of the new layer\n",
    "        self.architecture.append({\"inputSize\": inputSize,\n",
    "                                  \"outputSize\": outputSize, \n",
    "                                  \"activation\": activation})\n",
    "        \n",
    "        # Initialize parameters for the new layer\n",
    "        layerIndex = len(self.architecture)\n",
    "        self.paramValues[\"W\"+str(layerIndex)] = np.random.randn(outputSize, inputSize) * 0.1\n",
    "        self.paramValues[\"b\"+str(layerIndex)] = np.random.randn(outputSize, 1) * 0.1\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def sigmoid_deriv(self, dA, z):\n",
    "        sig_z = sigmoid(z)\n",
    "        return dA * sig_z * (1 - sig_z)\n",
    "\n",
    "    def relu_deriv(self, dA, z):\n",
    "        dz = np.array(dA, copy=True)\n",
    "        dz[z <= 0] = 0\n",
    "        return dz\n",
    "    \n",
    "    def forward_calculation(self):\n",
    "        self.layerValues = {}\n",
    "        A = self.X\n",
    "        layerIndex = 1\n",
    "        for layer in self.architecture:\n",
    "            self.layerValues[\"A\"+str(layerIndex-1)] = A\n",
    "            \n",
    "            if layer[\"activation\"] == \"relu\":\n",
    "                activation_func = self.relu\n",
    "            elif layer[\"activation\"] == \"sigmoid\":\n",
    "                activation_func = self.sigmoid\n",
    "            \n",
    "            W = self.paramValues[\"W\"+str(layerIndex)]\n",
    "            b = self.paramValues[\"b\"+str(layerIndex)]\n",
    "            Z = np.dot(W, A) + b\n",
    "            self.layerValues[\"Z\"+str(layerIndex)] = Z\n",
    "            A = activation_func(Z)\n",
    "            \n",
    "            layerIndex += 1\n",
    "        \n",
    "        self.Y_hat = A\n",
    "        \n",
    "    def back_propagation(self):\n",
    "        self.gradientValues = {}\n",
    "        self.Y = self.Y.reshape(self.Y_hat.shape)\n",
    "        layerIndex = len(self.architecture)\n",
    "        \n",
    "        dA_lastLayer = (1 - self.Y)/(1 - self.Y_hat) - self.Y/self.Y_hat\n",
    "        \n",
    "        for layer in reversed(self.architecture):\n",
    "            A_lastLayer = self.layerValues[\"A\"+str(layerIndex-1)]\n",
    "            Z = self.layerValues[\"Z\"+str(layerIndex)]\n",
    "            W = self.paramValues[\"W\"+str(layerIndex)]\n",
    "            b = self.paramValues[\"b\"+str(layerIndex)]\n",
    "            m = A_lastLayer.shape[1]\n",
    "            \n",
    "            if layer[\"activation\"] == \"relu\":\n",
    "                activation_func_deriv = self.relu_deriv\n",
    "            elif layer[\"activation\"] == \"sigmoid\":\n",
    "                activation_func_deriv = self.sigmoid_deriv\n",
    "                \n",
    "            dZ = activation_func_deriv(dA_lastLayer, Z)\n",
    "            db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "            dW = np.dot(dZ, A_lastLayer.T) / m\n",
    "            dA_lastLayer = np.dot(W.T, dZ)\n",
    "            \n",
    "            self.gradientValues[\"dW\"+str(layerIndex)] = dW\n",
    "            self.gradientValues[\"db\"+str(layerIndex)] = db\n",
    "            \n",
    "            layerIndex -= 1\n",
    "        \n",
    "    def update_params(self):\n",
    "        for layerIndex in range(1, len(self.architecture)+1):\n",
    "            self.paramValues[\"W\"+str(layerIndex)] -= self.learning_rate * self.gradientValues[\"dW\"+str(layerIndex)]\n",
    "            self.paramValues[\"b\"+str(layerIndex)] -= self.learning_rate * self.gradientValues[\"db\"+str(layerIndex)]\n",
    "            \n",
    "    def train(self, X, Y, epochs, learning_rate):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cost_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            self.forward_calculation()\n",
    "            self.cost_history.append(calculate_cost(self.Y_hat, self.Y))\n",
    "            self.accuracy_history.append(calculate_accuracy(self.Y_hat, self.Y))\n",
    "            self.back_propagation()\n",
    "            self.update_params()\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Epoch {i}: Cost = {cost:.2f}, Accuracy = {accuracy:.2f}\".format(i=i, cost=self.cost_history[-1],\n",
    "                                                                                      accuracy=self.accuracy_history[-1]))\n",
    "    \n",
    "    def predict(self, X_pred):\n",
    "        A = X_pred\n",
    "        layerIndex = 1\n",
    "        for layer in self.architecture:           \n",
    "            if layer[\"activation\"] == \"relu\":\n",
    "                activation_func = self.relu\n",
    "            elif layer[\"activation\"] == \"sigmoid\":\n",
    "                activation_func = self.sigmoid\n",
    "            \n",
    "            W = self.paramValues[\"W\"+str(layerIndex)]\n",
    "            b = self.paramValues[\"b\"+str(layerIndex)]\n",
    "            Z = np.dot(W, A) + b\n",
    "            self.layerValues[\"Z\"+str(layerIndex)] = Z\n",
    "            A = activation_func(Z)\n",
    "            \n",
    "            layerIndex += 1\n",
    "        \n",
    "        return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will need a way to test the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(Y_hat, Y):\n",
    "    Y_hat_copy = np.copy(Y_hat)\n",
    "    Y_hat_copy[Y_hat_copy > 0.5] = 1\n",
    "    Y_hat_copy[Y_hat_copy < 0.5] = 0\n",
    "    on_top = (Y_hat_copy.astype(int) == Y)\n",
    "    return sum(on_top[0])/len(on_top[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Titanic\n",
    "\n",
    "We will now test our implementation with the Titanic dataset from Kaggle. \n",
    "\n",
    "For the pre-processing of the data, we will use an adaptation of the method described in the following tutorial: \n",
    "https://www.kaggle.com/jamesleslie/titanic-neural-network-for-beginners. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 9) (891,)\n",
      "(713, 9) (178, 9) (713, 1) (178, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "df_train_full = pd.read_csv(\"train.csv\", index_col=\"PassengerId\")\n",
    "df_test = pd.read_csv(\"test.csv\", index_col=\"PassengerId\")\n",
    "\n",
    "\n",
    "# We won't be using these now\n",
    "df_train = df_train_full.drop(['Name', 'Ticket', 'Cabin', 'Survived'], axis=1)\n",
    "\n",
    "# we need sex to be int\n",
    "for dataset in (df_train, df_test):\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n",
    "    dataset['Embarked'] = pd.Categorical(dataset['Embarked'])\n",
    "\n",
    "df_train = pd.concat([df_train, pd.get_dummies(df_train['Embarked'], prefix='Embarked')], axis=1)\n",
    "df_test = pd.concat([df_test, pd.get_dummies(df_test['Embarked'], prefix='Embarked')], axis=1)\n",
    "del df_train['Embarked']\n",
    "del df_test['Embarked']\n",
    "    \n",
    "# The lazy version for filling the missing ages\n",
    "df_train.Age.fillna(df_train.Age.mean(), inplace=True)\n",
    "\n",
    "# Scale continuous features\n",
    "scaler = StandardScaler()\n",
    "features_to_scale = ['Age', 'Fare', 'Parch', 'Pclass']\n",
    "\n",
    "for var in features_to_scale:\n",
    "    df_train[var] = df_train[var].astype('float64')\n",
    "    df_train[var] = scaler.fit_transform(df_train[var].values.reshape(-1, 1))\n",
    "    \n",
    "\n",
    "X = df_train\n",
    "y = df_train_full['Survived'].values\n",
    "print(X.shape, y.shape)\n",
    "y = y.reshape(y.size, 1)\n",
    "\n",
    "# split the data to get accuracy\n",
    "eighty = round(len(X) * .8)\n",
    "X_train, X_test = X[:eighty], X[eighty:]\n",
    "y_train, y_test = y[:eighty], y[eighty:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cost = 0.68, Accuracy = 0.61\n",
      "Epoch 1000: Cost = 0.67, Accuracy = 0.61\n",
      "Epoch 2000: Cost = 0.43, Accuracy = 0.81\n",
      "Epoch 3000: Cost = 0.40, Accuracy = 0.83\n",
      "Epoch 4000: Cost = 0.39, Accuracy = 0.84\n",
      "Epoch 5000: Cost = 0.38, Accuracy = 0.85\n",
      "Epoch 6000: Cost = 0.37, Accuracy = 0.86\n",
      "Epoch 7000: Cost = 0.36, Accuracy = 0.86\n",
      "Epoch 8000: Cost = 0.35, Accuracy = 0.86\n",
      "Epoch 9000: Cost = 0.33, Accuracy = 0.86\n",
      "CPU times: user 23.3 s, sys: 1.77 s, total: 25.1 s\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Let's see how the network performs!\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add_layer(9, 9, \"relu\")\n",
    "nn.add_layer(9, 9, \"relu\")\n",
    "nn.add_layer(9, 5, \"relu\")\n",
    "nn.add_layer(5, 1, \"sigmoid\")\n",
    "nn.train(X_train.T, y_train.T, 10000, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 178) (1, 178)\n",
      "Accuracy on test data:  0.85\n"
     ]
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test.T)\n",
    "\n",
    "print(y_test.T.shape, y_pred.shape)\n",
    "print(\"Accuracy on test data: \", round(calculate_accuracy(y_pred, y_test.T), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(nn.epochs), nn.accuracy_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we got a very decent accuracy of 0.86 on the training set and 0.85 on the test data.\n",
    "This could probably be improved with some work on tuning the parameters, but for the purposes of this assignment it is satisfactory as it shows that our implementation of a network is functional.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "Chapter 6 of Goodfellow, I., Bengio, Y., & Courville, A. (2016).Deep learning. MIT press.Chicago Retrieved from  http://www.deeplearningbook.org/contents/mlp.html \n",
    "\n",
    "** Tutorials **\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "* https://towardsdatascience.com/neural-net-from-scratch-using-numpy-71a31f6e3675\n",
    "* https://www.kaggle.com/jamesleslie/titanic-neural-network-for-beginners"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
